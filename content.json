{"pages":[{"title":"关于","text":"本人在西安交通大学完成了本科以及研究生的学习，感兴趣的领域为计算机视觉、模式识别、量化交易等。 2017年深圳禾思众成科技有限公司联合创始人，负责算法研发工作。 2020年加入新国都，负责医保业务综合服务终端的研发、送检和推广工作 目前致力于构建一条永生之路~","link":"/about/index.html"}],"posts":[{"title":"海思NNIE之PFPLD训练与量化","text":"本文首发于我的知乎：https://zhuanlan.zhihu.com/p/120376337 之前写了关于海思NNIE的一些量化部署工作，笔者不才，文章没有写得很具体，有些内容并没有完全写在里面。好在目前看到了一些使用nniefacelib脱坑的朋友，觉得这个工程还是有些用的。为了完善这个工程，最近也增加一些一站式的解决方案。开始正题吧！ https://github.com/hanson-young/nniefacelib 1. 训练 PFLD 是一个精度高、速度快、模型小三位一体的人脸关键点检测算法。github上也有对其进行的复现工作，而这次要介绍的就是https://github.com/hanson-young/nniefacelib/blob/master/PFPLD/README.md 。 PFPLD （A Practical Facial Pose and Landmark Detector），对PFLD的微改版本，笔者对其进行了一些微小的改变，名字中间多了个”P“。其实是对pose branch进行了加强，同时让其关键点对遮挡、模糊、光照等复杂情况更加鲁棒。 黄色虚线囊括的是主分支网络，用于预测关键点的位置；绿色虚线囊括的是head pose辅助网络。在训练时预测人脸姿态，从而修改loss函数，使更加关注那些稀有的，还有姿态角度过大的样本，从而提高预测的精度。同等规模的网络，只要精度上去，必然是可以想到很多办法来降低计算量的。 直观感受，这个loss的设计模式本质上是一种对抗数据不均衡的表达，和focal loss思想是一致的。但这类思想并不是对于每种工作都能work，笔者曾经回答过类似的问题。 深度学习的多个loss如何平衡 &amp; 有哪些「魔改」损失函数，曾经拯救了你的深度学习模型？ 接下来将介绍一些笔者对其微改的地方： 在github上的代码分为了两个分支，下面单独做一下讲解 二、V1.1.1分支 用PRNet（https://github.com/YadiraF/PRNet）标注人脸图像的姿态数据，比原始通过solvePNP得到的效果要好很多，这也直接增强了模型对pose的支持。PRNet是一个非常优秀的3D人脸方面的项目。论文也写的很精彩，强烈推荐去看。目前在活体检测领域用其渲染的depth map作为伪标签进行训练，已经成为了一种标配性的存在。所以当人脸姿态估计算法性能接近于它，证明训练的姿态已经非常不错了。如果想要得到更好的表现，用更加特殊的方法采集人脸姿态数据进行炼丹也是行得通的（吐槽：大部分开源姿态数据标注规范并不统一）。 在整个实验中pfld loss收敛速度比较慢，慢也是有原因的，过于重点关注少量复杂的样本，会使得对整体的grad调节不明显，因此对多种loss（mse, pfld, smoothl1 and wing）进行了对比，结果得出，wing loss的效果更加明显，收敛速度也比较快。 改进了pfld网络结构，让关键点和姿态角度都能回归的比较好，将landmarks branch合并到pose branch中。由于两个任务相关性较强， 这样做可以让其更加充分的影响。对于这种多任务之间正向促进的例子，通过对网络结构以及辅助监督信号的改进，可以使其结果并不会太过于依赖loss函数的设计。这并不是笔者在的主观判断，感兴趣，可以参考我之前的一个回答，如有不同之处，欢迎一起讨论相关话题。 深度学习的多个loss如何平衡 &amp; 有哪些「魔改」损失函数，曾经拯救了你的深度学习模型？ 三，master分支 分支V1.1.1也存在一些问题，比如最大的一个问题就是闭眼的时候效果并不好，显然眼睛部分的关键点已经出现过拟合了。而master分支改进的效果为右列，得到了一些优化。 我们也发现一个规律，分别用WLFW 98个点，LAPA 106个点的数据集进行训练，闭眼效果都不行，而300WLP上的却没问题。这或许是一个通病，我也试了其他的算法，也有这些问题，比如 https://github.com/zeusees/HyperLandmark 原版本的PFLD 为什么会出现这个现象呢？这其实和训练数据集里面闭眼图片的数量过少有关系，加强眼部的训练并不能抵抗这种情况，因为不是一个维度的事情，最佳的方式依然是添加闭眼数据。同时也建议大家在制作数据集的时候考虑数据的均衡性 详细的讨论如下： https://github.com/hanson-young/nniefacelib/issues/13 另外一个问题是PRNet的pose预测在抬头时候不精确，因此V1.1.1中直接用PRNet去标注也不是一种理想方式 为了解决上面两个后面发现的问题，为了解决数据的均衡性，我们挑选出LAPA 106个关键点的数据集中闭眼的数据，加入WLFW中用于解决闭眼问题，引入300WLP用于解决pose问题），新的代码对 dataloader以及wing loss函数进行了优化，目前数据集已经整理好放出来了！请使用PFPLD-Dataset数据集进行训练！欢迎尝鲜！ 虽然融合大法好，但是，我们发现一个问题，网上已经开源了很多的关键点数据集，而这些数据集的标注点数并不一致，有没有办法将这些数据都利用起来呢？比如上述WLFW 98点、LAPA 106点、300WLP 68点。答案是肯定的，我们可以有针对性的选择其中的一些点对眼睛、鼻子、轮廓进行单独加强！在实际操作上，LAPA相对于WLFW增加了部分关键点，而且是完全兼容的，这样就可以先对WLFW和LAPA进行一个融合，姑且叫做WLFW2。 而融合300WLP就会面临一些困难，因为点的对应上没法完全兼容，因此需要参考标注标准挑选一些能兼容的点进行训练。下面是对loss的一些修改。 class WingLoss(nn.Module): def __init__(self): super(WingLoss, self).__init__() self.num_lds = 98 self.size = self.num_lds * 2 self.w = 10.0 self.s = 5.0 # 挑选出WLFW2中的眼睛关键点 self.eye_index = [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75] # 挑选出300WLP中与WLFW2想对应的部分关键点 self.pts_68_to_98 = [33,34,35,36,37,42,43,44,45,46,51,52,53,54,55,56,57,58,59,60,64,68,72,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95] self.pts_onehot = [i for i in range(98)] for i in self.pts_onehot: if i in self.pts_68_to_98: self.pts_onehot[i] = True else: self.pts_onehot[i] = False self.epsilon = 2.0 def forward(self, attribute_gt, landmark_gt, euler_angle_gt, type_flag , angle, landmarks, train_batchsize): landms_const = torch.tensor(-2).cuda() pose_68landms_const = torch.tensor(0).cuda() # WLFW2中98个点的loss pos1 = type_flag == landms_const landm_p = landmarks.reshape(-1, self.num_lds, 2)[pos1] landm_t = landmark_gt.reshape(-1, self.num_lds, 2)[pos1] lds_98_loss = 0 if landm_p.shape[0] &gt; 0: x = landm_t - landm_p c = self.w * (1.0 - math.log(1.0 + self.w / self.epsilon)) absolute_x = torch.abs(x) weight_attribute = landm_p*0.0 + 1.0 weight_attribute[:,self.eye_index] *= self.s absolute_x = torch.mul(absolute_x, weight_attribute) lds_losses = torch.where(self.w &gt; absolute_x, self.w * torch.log(1.0 + absolute_x / self.epsilon), absolute_x - c) lds_98_loss = torch.mean(torch.sum(lds_losses, axis=[1, 2]), axis=0) # 300WLP 中部分关键点的loss和姿态的loss pos2 = type_flag == pose_68landms_const pose_p = angle.view(-1, 3)[pos2] pose_t = euler_angle_gt.view(-1, 3)[pos2] pose_loss = 0 if pose_p.shape[0] &gt; 0: pose_loss = F.smooth_l1_loss(pose_p, pose_t, reduction='mean') landm_p = landmarks.reshape(-1, self.num_lds, 2)[pos2] landm_t = landmark_gt.reshape(-1, self.num_lds, 2)[pos2] lds_68_loss = 0 if landm_p.shape[0] &gt; 0: landm_p = landm_p[:, self.pts_onehot] landm_t = landm_t[:, self.pts_onehot] x = landm_t - landm_p absolute_x = torch.abs(x) c = self.w * (1.0 - math.log(1.0 + self.w / self.epsilon)) lds_losses = torch.where(self.w &gt; absolute_x, self.w * torch.log(1.0 + absolute_x / self.epsilon), absolute_x - c) lds_68_loss = torch.mean(torch.sum(lds_losses, axis=[1, 2]), axis=0) return lds_98_loss + lds_68_loss, pose_loss*1000 疫情当下，口罩遮挡，玄学优化，美图共赏 四、量化 过去一周，笔者对训练代码进行了整理，完成了多种版本的转换工作，包括 pytorch caffe ncnn nnie 听说有小伙伴将这套模型跑到了ios上，说不定之后会放出来。 扯了一大堆，那开始介绍下本文最核心的NNIE。有首先我们要选择一个比较优秀的训练框架，比如，我们选择了pytorch。然后要将模型转换为caffe，那我们选择了onnx作为过度环节 python convert_to_onnx.pypython3 -m onnxsim ./models/onnx/checkpoint_epoch_final.onnx ./models/onnx/pfpld.onnxcd cvtcaffepython convertCaffe.py 可以看得出来，经过了很少的步骤，一个被图优化过的caffe模型就出来了，包括merge bn，inplace等优化，&quot;工具人&quot;onnx在其中起到了很重要的作用。 之前也提到过batchnorm会对精度造成一些不知所以的影响，所以 无论我们遇到什么困难，都不要怕，微笑着面对它，消除恐惧的最好办法就是避开恐惧，避开，才会胜利，加油，奥利给！ 另外，笔者自己训练模型的时候是不会考虑减均值这种操作的，只会做data_scale处理，为什么这么做，因为放弃思考真得很香。记性不好，不想遇到问题的时候去查，也不太相信减均值能带来明显收益！","link":"/2021/01/04/%E6%B5%B7%E6%80%9DNNIE%E4%B9%8BPFPLD%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%87%8F%E5%8C%96/"},{"title":"海思NNIE之RetinaFace量化部署","text":"本文首发于我的知乎：https://zhuanlan.zhihu.com/p/111399987 0. 前言 接上一期 海思NNIE之Mobilefacenet量化部署 文章。 关于上述内容，还是得到了一些认可，索性把人脸全家福奉上了，Github地址如下： https://github.com/hanson-young/nniefacelib nniefacelib是一个在海思35xx系列芯片上运行的人脸算法库，目前集成了mobilefacenet和retinaface。 后期也会融合一些其他经典的模型，目的也是总结经验，让更多人早日脱离苦海。欢迎关注！ 这篇的话，就讲下RetinaFace的量化和部署吧！相信很多同学在转换的时候吃了苦头，那我们就来宣泄一下吧！ RetinaFace是目前非常优秀的开源人脸检测算法 https://github.com/deepinsight/insightface/tree/master/RetinaFace 实测效果确实很棒，鲁棒性强，关键点准，能够识别比较复杂场景下的人脸，甚至于比我采用私有数据集训练的mtcnn某些方面还要强，有全局感受野的模型在复杂人脸上的检测效果会有很大优势。很多同学在海思上也有往mtcnn方向下功夫的，其实可以走得通，但现实是，海思弱鸡的CPU算力还是留给其他业务逻辑吧，直接用one-stage的方法也是很香的。另外one stage模型优化起来也是很粗暴的。天下武功，唯快不破，tracking也可以告别光流，而去使用sort，甚至一些场景直接用deepsort去解决switch IDs。综上，将RetinaFace这类模型放上海思的板子上是有巨大的优势。 我参考的代码和模型来自于 https://github.com/Charrin/RetinaFace-Cpp 1. 特殊层Crop 在nnie上，有的层是不支持的，这个官方SDK里有专门的章节描述，其中retinaface就有一个Crop层是不支持的，需要手动写插件，但这当然是不能接受的，毕竟一想到将计算量转移部分到CPU上就觉得不踏实，后期肯定会给自己带来麻烦。那么对于这个问题，我们首先要做的就是明白这个层的作用。 Caffe中crop_layer描述是这样的： 数据以blobs形式存在，blob是一个四维结构的Tensor（batch size, channels, height, width），假设crop_layer的输入（bottom）有两个，A和B，输出（top）为C A——要进行裁剪的bottom，假设A的size为（20，50，512，512） B——裁剪的参考输入，size为（20，10，256，256） C——输出top，它是在A的基础上按照B的size裁剪而来，所以输出C的size和B是一样的 layer { name: &quot;crop0&quot; type: &quot;Crop&quot; bottom: &quot;A&quot; bottom: &quot;B&quot; top: &quot;C&quot;} 知道Crop的原理和模型训练的流程就能明白，作者利用Crop的原理其实就是解决多尺度输入的问题，可以在比赛或实验中去利用多尺度提升性能，但实际在跑视频流或者做应用的时候并不需要每一次都跑尺寸不同的图像，一般都是根据需要，设定单一尺度。 所以我采取了比较暴力的做法，就是直接在mnet.prototxt中两处地方去除了Crop层（Fig.2.2，Fig.2.3），然后寻找几组固定的输入比如512x512、640x640、768x768、1024x1024等，只要保证在Crop操作之前两个层的维度是一致的就没有什么问题！ 然后去掉prototxt里的两处crop后，结构图如下： 修改完prototxt后就可以直接在RuyiStudio中使用Marker进行模型拓扑图像的观测，调节data的输入尺度，可以测试出几组能满足条件的size，这样之后的转换问题应该就不会很大了！如Figure2.4和Figure2.5所示。 3. ReShape prototxt中存在多处reshape的地方，如下图，需要将dim: 1改成dim: 0，原因很简单，在量化的时候会报一个错误，就是只能设置CHW三个维度，没有N，这个维度的设定应该是为了让NNIE多张输入的时候方便操作四维数据，可以参考样例中fasterRCNN中的写法，进行必要调整。 4. 小雷区 还有一个地方比较坑，也是CNN_convert_bin_and_print_featuremap.py代码的问题，因为这个代码里面没有将BGR转化为RGB，而cfg里写的并没有用到，之前测试的图像是agedb中的图像，刚好是一张三通道一致的灰度图，因此需要在读取图像的时候注意一下，加上这句代码 img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) 5. 大雷区 开开心心的走到了这一步，那其实最大的问题才刚刚抵达战场。就是前方这个错误，让人魂牵梦萦的Error。最后的解决方案也让人无法入睡。 其实解决方法就是改变prototxt中layer的顺序，对你没听过。修改过后的prototxt已经被放在nniefacelib了 其中有几个点是目前还想不明白的，非常希望得到大佬的指点： pycaffe上运行确实是没问题的，结果也正确。NNIE mapper工具可以生成inst.mk，生成过程中没有如上错误，但是在板端测试的时候输出是错误的；NNIE mapper工具也无法生成func.mk，但可以生成mapper_quant，生成过程中有如上错误。 有的地方batchnorm层之前精度还是很高的，过batchnorm后精度就完全对不上了，有的地方没问题，本以为是batchnorm里的参数有问题，但和mxnet版本的model param进行了对比后并没有发现任何异常。最后输出层的结果却也是精损极小，意料之中。这个问题在转其他模型的时候也遇到过。（PS：要不直接merge bn吧，这个问题就可以跳过了） 经过逐层的调试虽然解决了模型生成的问题，但是也仅仅改变了prototxt layer的顺序，按道理说并不应该出现这样的情况。 虽然存在一些不可告知的问题，但是也给了我们一个调试经验，在遇到输出不对的时候不要慌，先从对的那一层开始，修改prototxt，删除之后的layer开始调试，直到查明原因为止。如果还是没办法，重新训练也是个好方法。 5. 量化仿真 做完了上面的步骤就可以进行量化了，目前做的测试显示，低精度的量化在landmarks和bbox的回归上精度还是偏低的，但够用，部署起来输出的误差也就像素级误差，而高精度模式下自然也不用担心，只是效率会稍微降低一点，我也给出一个在3516DV300上的大概结果吧！图像尺寸640x640，算上后处理，高精度大概40ms，低精度20多ms。 如果对文章有什么不理解或者疑惑的地方，欢迎到文章开头的知乎文章的讨论区给我留言哦。","link":"/2020/03/06/%E6%B5%B7%E6%80%9DNNIE%E4%B9%8BRetinaFace%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"title":"海思NNIE之Mobilefacenet量化部署","text":"本文首发于我的知乎：https://zhuanlan.zhihu.com/p/107548509 0. 前言 那就直接更新下整理好的代码链接吧！ https://github.com/hanson-young/nniefacelib 当您点进这篇文章，我想肯定不需要过多的去向您介绍华为海思35xx系列芯片的型号参数或者强大之处。另外这个教程也是建立已经配置好环境，并掌握Ruyi Studio的基本使用前提下的。如果还没有跑过其中的一些sample，网上也有一些教程，推荐看刘山老师的博客（地址为：https://blog.csdn.net/avideointerfaces/article/details/88585654）。 这篇文章的目录如下： 简介 目录结构 mobilefacenet.cfg文件的配置 生成NNIE mk模型 Vector Comparision NNIE mobilefacenet板上特征提取 附录 1. 简介 海思35xx系列芯片对比起nvidia TX2、Intel Movidius神经计算棒等一众边缘计算产品，有其惊艳的地方，因其集成了强大的算力模块，集成度和功能模块齐全，最重要的是成本低，成为了安防行业的首选芯片。但是也有一些麻烦的地方，主要是在于其开发难度的提高，大家都是摸着石头过河（3288老玩家转行也是能体会到痛苦的）。在转自己的模型时，坑比想象的要多，并且海思官方SDK也存在一些错误之处，让人很难捉摸，所以有时候需要自己多去独立思考。这次我记录了在转换人脸识别模型mobilefacenet（https://github.com/deepinsight/insightface）下了比较坑的三个点，毕竟是个新玩意儿，多半是版本发布时候不统一造成的： CNN_convert_bin_and_print_featuremap.py 代码出现错误，cfg中的【image_list】这个字段并没有在代码中出现，代码中只有【image_file】，因此需要修改这一地方。 CNN_convert_bin_and_print_featuremap.py和Get Caffe Output这里的预处理方式都是先乘以【data_scale】，再减均值【mean_file】，而在量化生成 .mk 文件时却是先减均值再乘以scale的。 量化需要使用多张图片，而CNN_convert_bin_and_print_featuremap.py各层产生的feature仅仅是一张图片，这在做【Vector Comparision】时候就难以清楚的明白到底最后mk文件是第几张图像。 2. 目录结构 3. mobilefacenet.cfg文件的配置 可以从github上下载mxnet2caffe的mobilefacenet模型（https://github.com/honghuCode/mobileFacenet-ncnn/tree/feature/mobilefacenet-mxnet2caffe），首先需要修改mobilefacenet.prototxt（https://github.com/honghuCode/mobileFacenet-ncnn/blob/feature/mobilefacenet-mxnet2caffe/mobilefacenet.prototxt）的输入层以符合NNIE caffe网络的结构标准： 而量化mk使用的【mean_file】pixel_mean.txt是特别需要注意的 我从agedb_30人脸数据库里面挑选了10张图像来做量化处理，为什么需要多张量化，请参考文章https://zhuanlan.zhihu.com/p/58182172，我们选择【10.jpg】来做 【Vector Comparision】，其实就是imageList.txt里的排列在最后的那张图片 具体配置如下： [prototxt_file] ./mark_prototxt/mobilefacenet_mark_nnie_20190723102335.prototxt[caffemodel_file] ./data/face/mobilefacenet.caffemodel[batch_num] 256[net_type] 0[sparse_rate] 0[compile_mode] 0[is_simulation] 0[log_level] 3[instruction_name] ./data/face/mobilefacenet_inst[RGB_order] RGB[data_scale] 0.0078125[internal_stride] 16[image_list] ./data/face/images/imageList20190723102419.txt[image_type] 1[mean_file] ./data/face/pixel_mean.txt[norm_type] 5 4. 生成NNIE mk模型 Start [RuyiStudio Wk NNIE Mapper] [E:\\Code\\nnie\\windows\\RuyiStudio-2.0.31\\workspace\\HeilsFace\\mobilefacenet.cfg] HeilsFace (2019-07-23 10:48:17)Mapper Version 1.1.2.0_B050 (NNIE_1.1) 1812171743151709begin net parsing.....end net parsingbegin prev optimizing........end prev optimizing....begin net quantalizing(GPU)........................**********************************************************WARNING: file: Inference::computeNonlinearQuantizationDelta line: 92data containing only zeros; set max value to 1e-6.**********************************************************WARNING: file: Inference::computeNonlinearQuantizationDelta line: 92data containing only zeros; set max value to 1e-6........................................end quantalizingbegin optimizing.....end optimizingbegin NNIE[0] mem allocation.......end NNIE[0] memory allocatingbegin NNIE[0] instruction generating.................end NNIE[0] instruction generatingbegin parameter compressing.....end parameter compressingbegin compress index generating....end compress index generatingbegin binary code generating............................................................................................................................................................................................................................................................................................................................................................end binary code generatingbegin quant files writing....end quant files writing===============E:\\Code\\nnie\\windows\\RuyiStudio-2.0.31\\workspace\\HeilsFace\\mobilefacenet.cfg Successfully!=============== 结束之后会生成： mobilefacenet_inst.wk文件 mapper_quant文件夹，里面有量化输出的结果，如图 Fig.4.1，也就是./data/face/images/10.jpg 记住，mk量化过程在【mapper_quant】文件夹中生成的features是最后一张图片的inference结果，这也是文章最开始说的第三个存在问题的地方 5. Vector Comparision 这一步，主要就是对比量化前后模型输出的精度损失，最重要的就是要debug一遍CNN_convert_bin_and_print_featuremap.py 因为这个脚本里确实藏了很多雷，我们先要比较原框架原模型inference的结果与这一脚本得出来的结果是否一致，如果存在不一致的情况，需要去核查一遍原因 文章开篇说到的第一个问题点 CNN_convert_bin_and_print_featuremap.py 中加载了mobilefacenet.cfg文件，但脚本中并不存在【image_list】这个字段，取而代之的是【image_file】这个字段 生成NNIE mk中，mobliefacenet.cfg 的【image_list】： CNN_convert_bin_and_print_featuremap.py 中加载.cfg代码片段： 因此需要根据实际情况修改 mobliefacenet.cfg ，这里最好是复制一份新的，旧的用于生成NNIE wk，在复制后的mobliefacenet.cfg中修改一下： 另外，我们需要特别注意预处理这一个环节，如文章开篇所阐述的第二点 我们注意到这里，data是uint8类型的array，是先乘以了【data_scale】的，也就是说和NNIE 生成wk中的操作顺序是不一致的。 (data - 128.0) * 0.0078125 &lt;==&gt; data * 0.0078125 - 1 因此这里需要做的修改就是需要将【mean_file】pixel_mean.txt修改为 修改完以上，然后直接运行代码，将最终模型提取的features fc1_output0_128_caffe.linear.float和caffe_forward.py（https://github.com/honghuCode/mobileFacenet-ncnn/blob/feature/mobilefacenet-mxnet2caffe/caffe_forward.py）中的进行比对，如果以上都没问题，可以看到结果是几乎一致的 caffe_forward.py生成的结果： [-0.82475293 -0.33066949 -0.9848339 2.44199681 0.41715512 0.67809981 0.29879519 1.14293635 -0.42905819 0.32940909 -1.20455348 1.01217067 0.83146936 -0.84349883 -1.49177814 -0.91509151 -1.39441037 0.00413842 0.97043389 -1.77688181 0.28639579 -1.06645989 -0.8570649 -2.09743094 -0.1394622 -1.15035641 -0.81590587 -3.93798804 -0.35600579 1.90367532 1.27935755 -2.07778478 -0.42563218 0.06624207 1.02597868 -0.52002895 -0.905873 -0.41364694 -1.40032899 -1.37654066 0.03066693 -0.18659458 -1.53931415 -0.55896652 2.42570448 -0.3044413 0.18183242 0.50442797 -2.36735368 -0.12376076 0.15200013 0.13939141 0.56305337 -0.10047323 1.50704932 0.05429612 -1.97527623 -0.75790995 1.89399767 0.56089604 -2.34883094 0.22600658 1.00399816 -0.55099922 1.77083731 0.10722937 2.21140814 0.06182361 0.03354079 0.97481596 -2.00423741 0.73168194 -1.79977489 -0.85182911 -0.06020565 -0.14835797 -1.93012297 -3.09269047 -0.60087907 -1.02915597 1.40985525 1.85411906 -1.21282506 -2.53264689 -0.63467324 -1.15255475 -0.59994221 0.21181655 1.30336523 -1.73625863 0.00861333 0.99906266 1.90666902 0.51179212 0.62143475 1.01997399 -1.65181398 1.55190873 0.43448481 -0.85371047 -0.68216199 1.28038061 0.4629558 -0.59671575 1.00122356 1.74233603 1.50384009 0.49827856 0.67030573 -1.20388556 1.00168729 -0.71768999 1.06416941 -2.55346298 -1.85579956 -2.18774438 -1.79652691 1.50856853 2.10628557 1.12313557 2.76396179 0.60242128 0.0550903 -1.31998527 -0.6896565 -0.07160443 1.21242583 -1.06733179] CNN_convert_bin_and_print_featuremap.py生成的结果（由于特征值太多，就不一一打印出来了）： 然后再生成，并进行【Vector Comparision】，量化终于成功了 6. NNIE mobilefacenet板上特征提取 做完了模型的量化，就可以进行仿真或者是在板子上进行实际测试了，这一步的坑并不是很多，主要还是得靠一些编程技巧了，建议熟悉C语言，这部分要熟悉sample代码，如果说非常熟悉c/c混编，也可以使用c。 6.1 修改例程 这里参考了https://blog.csdn.net/u011728480/article/details/92069793，其写法几乎一致，如下Fig.6.1 Fig.6.2是我所修改的代码片段，找到smp/a7_linux/mpp/sample/svp/nnie/sample/sample_nnie.c中该函数 void SAMPLE_SVP_NNIE_Cnn(void) 只用修改了该函数的前后两处代码 我们调用了 SAMPLE_SVP_NNIE_PrintReportResult 函数输出两个结果报表文件，结果分析当中会用到 seg0_layer38_output0_inst.linear.hexseg0_layer3605_output0_inst.linear.hex 整段函数代码参见文章末尾【附录】 6.2 bgr文件的生成 注意到上文中我使用了pcSrcFile，这也是例程中主流的格式bgr，那么我们一般的图片都是.jpeg格式的，为了更好的利用NNIE，所以就需要利用opencv来转化以下。 首先.bgr文件是可以由opencv Mat转换的，但完成转换代码的编写之前我们必须清楚像素的空间排列顺序。注意，以下转换代码简单采用像素复制，并没有考虑优化，运行会比较慢！参考博客 .bgr ==&gt; BBBBBB…GGGGGG…RRRRRR cv::Mat ==&gt; BGRBGRBGR…BGRBGRBGR .bgr --&gt; cv::Mat /*bgr格式 转 cv::Mat代码 */int bgr2mat(cv::Mat&amp; img, int width, int height, int channel, const char* pth){ if (pth) { FILE* fp; unsigned char *img_data = NULL; unsigned char *img_data_conv = NULL; img_data = (unsigned char*)malloc(sizeof(unsigned char) * width * height * channel); //unsigned char img_data[300 * 300 * 3]; img_data_conv = (unsigned char*)malloc(sizeof(unsigned char) * width * height * channel); fp = fopen(pth, &quot;rb&quot;); if (!fp) { return 0; } fread(img_data, 1, width * height * channel, fp); fclose(fp); for (size_t k = 0; k &lt; channel; k++) for (size_t i = 0; i &lt; height; i++) for (size_t j = 0; j &lt; width; j++) img_data_conv[channel * (i * width + j) + k] = img_data[k * height * width + i * width + j]; img = cv::Mat(height, width, CV_8UC3, img_data_conv); //free(img_data_conv); //img_data_conv = NULL; free(img_data); img_data = NULL; return 1; } return 0;} cv::Mat --&gt;.bgr /*cv::Mat 转 bgr格式代码 */int mat2bgr(cv::Mat&amp; img, const char* bgr_path){ if (bgr_path) { FILE* fp = fopen(bgr_path, &quot;wb&quot;); int step = img.step; int h = img.rows; int w = img.cols; int c = img.channels(); std::cout &lt;&lt; step&lt;&lt; std::endl; for (int k = 0; k &lt; c; k++) for (int i = 0; i &lt; h; i++) for (int j = 0; j &lt; w; j++) { //两种写法 //fwrite(&amp;img.data[i*step + j * c + k], sizeof(uint8_t), 1, fp); fwrite(&amp;img.data[c*(i * w + j) + k], sizeof(uint8_t), 1, fp); } fclose(fp); //cv::Mat tmp; //bgr2mat(tmp, w, h, 3, bgr_path); //cv::imshow(&quot;tmp&quot;, tmp); //cv::waitKey(0); return 1; } return 0;} 6.3 模型额外问题 pc上运行 E:\\Code\\nnie\\software\\sample_simulator\\Release\\sample_simulator.exe 板上运行 /nfsroot/Hi3516CV500_SDK_V2.0.1.0/smp/a7_linux/mpp/sample/svp/nnie # ./sample_nnie_main 4 可能会出现如下（Fig.6.5，Fig.6.6）错误，原因是生成NNIE wk文件的mapper工具有版本要求，下面错误当中使用的nnie mapper 版本是V1.1.2.0，而指令仿真或者是板上的SDK是V1.2的，解决办法就是使用nnie mapper V1.2版本重新生成一下wk模型，如（Fig.6.7），生成inst/chip.wk的时间比较久，在我机器上大概要2个小时，因为inst.wk实际上是需要进行参数压缩和二进制代码生成，这可能也是inst.mk比func.wk文件大的原因（如Fig.6.8），而生成func.wk的时间会比较短，建议在PC上调试的时候选择func/simulation模型 6.4 运行结果及分析 修改完sample_nnie.c中的代码后，在宿主机上进行make，然后到海思板子上运行可执行文件即可 拷贝出生成的两个打印报表文件到Ruyi studio，进行比对测试 seg0_layer38_output0_inst.linear.hexseg0_layer3605_output0_inst.linear.hex 如Fig.6.10，Fig.6.11，虽然说板上和仿真情况下还是会有一定的差别，但总体的误差是比较小的，基本可以接受，如果无法接受，可以尝试int16模型 7. 附录 void SAMPLE_SVP_NNIE_Cnn(void){ HI_CHAR *pcSrcFile = &quot;./data/nnie_image/rgb_planar/10.bgr&quot;; HI_CHAR *pcModelName = &quot;./data/nnie_model/face/mobilefacenet_inst.wk&quot;; HI_U32 u32PicNum = 1; HI_S32 s32Ret = HI_SUCCESS; SAMPLE_SVP_NNIE_CFG_S stNnieCfg = {0}; SAMPLE_SVP_NNIE_INPUT_DATA_INDEX_S stInputDataIdx = {0}; SAMPLE_SVP_NNIE_PROCESS_SEG_INDEX_S stProcSegIdx = {0}; /*Set configuration parameter*/ stNnieCfg.pszPic= pcSrcFile; stNnieCfg.u32MaxInputNum = u32PicNum; //max input image num in each batch stNnieCfg.u32MaxRoiNum = 0; stNnieCfg.aenNnieCoreId[0] = SVP_NNIE_ID_0;//set NNIE core s_stCnnSoftwareParam.u32TopN = 5; /*Sys init*/ SAMPLE_COMM_SVP_CheckSysInit(); /*CNN Load model*/ SAMPLE_SVP_TRACE_INFO(&quot;Cnn Load model!\\n&quot;); s32Ret = SAMPLE_COMM_SVP_NNIE_LoadModel(pcModelName,&amp;s_stCnnModel); SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,CNN_FAIL_0,SAMPLE_SVP_ERR_LEVEL_ERROR, &quot;Error,SAMPLE_COMM_SVP_NNIE_LoadModel failed!\\n&quot;); /*CNN parameter initialization*/ /*Cnn software parameters are set in SAMPLE_SVP_NNIE_Cnn_SoftwareParaInit, if user has changed net struct, please make sure the parameter settings in SAMPLE_SVP_NNIE_Cnn_SoftwareParaInit function are correct*/ SAMPLE_SVP_TRACE_INFO(&quot;Cnn parameter initialization!\\n&quot;); s_stCnnNnieParam.pstModel = &amp;s_stCnnModel.stModel; s32Ret = SAMPLE_SVP_NNIE_Cnn_ParamInit(&amp;stNnieCfg,&amp;s_stCnnNnieParam,&amp;s_stCnnSoftwareParam); SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,CNN_FAIL_0,SAMPLE_SVP_ERR_LEVEL_ERROR, &quot;Error,SAMPLE_SVP_NNIE_Cnn_ParamInit failed!\\n&quot;); /*record tskBuf*/ s32Ret = HI_MPI_SVP_NNIE_AddTskBuf(&amp;(s_stCnnNnieParam.astForwardCtrl[0].stTskBuf)); SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,CNN_FAIL_0,SAMPLE_SVP_ERR_LEVEL_ERROR, &quot;Error,HI_MPI_SVP_NNIE_AddTskBuf failed!\\n&quot;); /*Fill src data*/ SAMPLE_SVP_TRACE_INFO(&quot;Cnn start!\\n&quot;); stInputDataIdx.u32SegIdx = 0; stInputDataIdx.u32NodeIdx = 0; s32Ret = SAMPLE_SVP_NNIE_FillSrcData(&amp;stNnieCfg,&amp;s_stCnnNnieParam,&amp;stInputDataIdx); SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,CNN_FAIL_1,SAMPLE_SVP_ERR_LEVEL_ERROR, &quot;Error,SAMPLE_SVP_NNIE_FillSrcData failed!\\n&quot;); /*NNIE process(process the 0-th segment)*/ stProcSegIdx.u32SegIdx = 0; s32Ret = SAMPLE_SVP_NNIE_Forward(&amp;s_stCnnNnieParam,&amp;stInputDataIdx,&amp;stProcSegIdx,HI_TRUE); SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,CNN_FAIL_1,SAMPLE_SVP_ERR_LEVEL_ERROR, &quot;Error,SAMPLE_SVP_NNIE_Forward failed!\\n&quot;); /*Software process*/ /*if user has changed net struct, please make sure SAMPLE_SVP_NNIE_Cnn_GetTopN function's input datas are correct*/ s32Ret = SAMPLE_SVP_NNIE_Cnn_GetTopN(&amp;s_stCnnNnieParam,&amp;s_stCnnSoftwareParam); SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,CNN_FAIL_1,SAMPLE_SVP_ERR_LEVEL_ERROR, &quot;Error,SAMPLE_SVP_NNIE_CnnGetTopN failed!\\n&quot;); /*Print result*/ SAMPLE_SVP_TRACE_INFO(&quot;Cnn result:\\n&quot;); s32Ret = SAMPLE_SVP_NNIE_Cnn_PrintResult(&amp;(s_stCnnSoftwareParam.stGetTopN), s_stCnnSoftwareParam.u32TopN); SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,CNN_FAIL_1,SAMPLE_SVP_ERR_LEVEL_ERROR, &quot;Error,SAMPLE_SVP_NNIE_Cnn_PrintResult failed!\\n&quot;); /*Print results*/ { printf(&quot;features:\\n{\\n&quot;); printf(&quot;stride: %d\\n&quot;,s_stCnnNnieParam.astSegData[0].astDst[0].u32Stride); printf(&quot;blob type :%d\\n&quot;,s_stCnnNnieParam.astSegData[0].astDst[0].enType); printf(&quot;{\\n\\tc :%d&quot;, s_stCnnNnieParam.astSegData[0].astDst[0].unShape.stWhc.u32Chn); printf(&quot;\\n\\th :%d&quot;, s_stCnnNnieParam.astSegData[0].astDst[0].unShape.stWhc.u32Height); printf(&quot;\\n\\tw :%d \\n}\\n&quot;, s_stCnnNnieParam.astSegData[0].astDst[0].unShape.stWhc.u32Width); HI_S32* ps32Score = (HI_S32* )((HI_U8* )s_stCnnNnieParam.astSegData[0].astDst[0].u64VirAddr); printf(&quot;blobs fc1:\\n[&quot;); for(HI_U32 i = 0; i &lt; 128; i++) { printf(&quot;%f ,&quot;,*(ps32Score + i) / 4096.f); } printf(&quot;]\\n}\\n&quot;); } s32Ret = SAMPLE_SVP_NNIE_PrintReportResult(&amp;s_stCnnNnieParam); SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret, CNN_FAIL_1, SAMPLE_SVP_ERR_LEVEL_ERROR,&quot;Error,SAMPLE_SVP_NNIE_PrintReportResult failed!&quot;);CNN_FAIL_1: /*Remove TskBuf*/ s32Ret = HI_MPI_SVP_NNIE_RemoveTskBuf(&amp;(s_stCnnNnieParam.astForwardCtrl[0].stTskBuf)); SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,CNN_FAIL_0,SAMPLE_SVP_ERR_LEVEL_ERROR, &quot;Error,HI_MPI_SVP_NNIE_RemoveTskBuf failed!\\n&quot;);CNN_FAIL_0: SAMPLE_SVP_NNIE_Cnn_Deinit(&amp;s_stCnnNnieParam,&amp;s_stCnnSoftwareParam,&amp;s_stCnnModel); SAMPLE_COMM_SVP_CheckSysExit();}","link":"/2020/03/04/%E6%B5%B7%E6%80%9DNNIE%E4%B9%8BMobilefacenet%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2/"}],"tags":[{"name":"nniefacelib","slug":"nniefacelib","link":"/tags/nniefacelib/"}],"categories":[{"name":"技术","slug":"技术","link":"/categories/%E6%8A%80%E6%9C%AF/"}]}