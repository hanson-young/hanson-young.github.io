{"pages":[{"title":"关于","text":"demo of hexo-theme-kaze","link":"/about/index.html"}],"posts":[{"title":"公式渲染","text":"简介 启用公式 本主题支持 mathjax 和 katex 两大渲染引擎，您可以在博客主题设置 latex 中 enable: true 启用。 更换渲染器 由于Hexo本身渲染器的冲突，有一部分公式无法渲染，建议采用其他渲染器，首先 npm uninstall hexo-renderer-marked -S 之后mathjax推荐hexo-renderer-kramed，katex推荐hexo-renderer-markdown-it-plus npm install hexo-renderer-kramed -Snpm install hexo-renderer-markdown-it-plus -S 注意，您只可以保留一个渲染器 演示 以下所有公式均通过 katex 渲染 整行公式 f(a)=12πi∮f(z)z−adzf(a) = \\frac{1}{2\\pi i} \\oint\\frac{f(z)}{z-a}dz f(a)=2πi1​∮z−af(z)​dz cos⁡(θ+ϕ)=cos⁡(θ)cos⁡(ϕ)−sin⁡(θ)sin⁡(ϕ)\\cos(\\theta+\\phi)=\\cos(\\theta)\\cos(\\phi)−\\sin(\\theta)\\sin(\\phi) cos(θ+ϕ)=cos(θ)cos(ϕ)−sin(θ)sin(ϕ) ∫D(∇⋅F)dV=∫∂DF⋅ndS\\int_D ({\\nabla\\cdot} F)dV=\\int_{\\partial D} F\\cdot ndS ∫D​(∇⋅F)dV=∫∂D​F⋅ndS ∇⃗×F⃗=(∂Fz∂y−∂Fy∂z)i+(∂Fx∂z−∂Fz∂x)j+(∂Fy∂x−∂Fx∂y)k\\vec{\\nabla} \\times \\vec{F} =\\left( \\frac{\\partial F_z}{\\partial y} - \\frac{\\partial F_y}{\\partial z} \\right) \\mathbf{i}+ \\left( \\frac{\\partial F_x}{\\partial z} - \\frac{\\partial F_z}{\\partial x} \\right) \\mathbf{j}+ \\left( \\frac{\\partial F_y}{\\partial x} - \\frac{\\partial F_x}{\\partial y} \\right) \\mathbf{k} ∇×F=(∂y∂Fz​​−∂z∂Fy​​)i+(∂z∂Fx​​−∂x∂Fz​​)j+(∂x∂Fy​​−∂y∂Fx​​)k σ=1N∑i=1N(xi−μ)2\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2} σ=N1​i=1∑N​(xi​−μ)2​ (∇XY)k=Xi(∇iY)k=Xi(∂Yk∂xi+ΓimkYm)(\\nabla_X Y)^k = X^i (\\nabla_i Y)^k =X^i \\left( \\frac{\\partial Y^k}{\\partial x^i} + \\Gamma_{im}^k Y^m \\right) (∇X​Y)k=Xi(∇i​Y)k=Xi(∂xi∂Yk​+Γimk​Ym) det(JG)=∂xd+1∂zd+1∂xd+2∂zd+2...∂xD∂zD=βd+1βd+2...βDdet(J_G) =\\frac{\\partial{x_{d+1}}}{\\partial{z_{d+1}}}\\frac{\\partial{x_{d+2}}}{\\partial{z_{d+2}}}...\\frac{\\partial{x_D}}{\\partial{z_D}} \\\\ =β_{d+1}β_{d+2}...β_{D} det(JG​)=∂zd+1​∂xd+1​​∂zd+2​∂xd+2​​...∂zD​∂xD​​=βd+1​βd+2​...βD​ 行内公式 求根公式是x=−b±b2−4ac2ax = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}x=2a−b±b2−4ac​​ E=mc2E=mc^2E=mc2是质能方程式 对于一个序列a1,a2,⋯ ,ama_1,a_2,\\cdots,a_ma1​,a2​,⋯,am​，有a1+a2+⋯+am=2⋅(a1⊕a2⊕⋯⊕am)a_1+a_2+\\cdots+a_m=2\\cdot(a_1\\oplus a_2\\oplus\\cdots\\oplus a_m)a1​+a2​+⋯+am​=2⋅(a1​⊕a2​⊕⋯⊕am​)","link":"/2020/07/27/latex/"},{"title":"代码高亮","text":"简介 高亮方案 本主题没有使用自定义高亮方式，高亮方案在Hexo本身支持的 highlightjs 和 prismjs 基础上修改得到 使用highlightjs highlight: enable: true line_number: false auto_detect: false tab_replace: '' wrap: true hljs: trueprismjs: enable: false ... highlight 和 prismjs 的 enable: true 只能保留一个 在站点配置文件将 highlight 设置 enable: true 即可开启，line_number 选项本主题暂不支持显示 若需要支持代码高亮可设置 hljs: true，该配色方案参考了 tomorrow night 等配色主题 其他具体参数设置可参考 Hexo相关文档 使用prismjs 在 prismjs 中设置 enable: true 和 preprocess: true 即可开启 默认支持 Line Numbers 和 Show Languages 两个插件 如果需要其他插件支持可以设置 preprocess: false 当前主题版本不支持 preprocess: false 自动引入相关文件 如有相关需求请自行引入相关 css 与 js 文件 设置 line_number: true 即可显示行号 演示 以下演示采用 prism，highlight 在主题效果可参见（TODO） 有八种高亮主题可选择 default coy dark funky okaidia solarizedlight tomorrow twilight Javascript const smoothScrollToTop = () =&gt; { let yTopValve = (window.pageYOffset || document.body.scrollTop || document.documentElement.scrollTop); if (yTopValve &gt; 1) { window.requestAnimationFrame(smoothScrollToTop); scrollTo(0, Math.floor(yTopValve * 0.85)); } else { scrollTo(0, 0); }};setTimeout(() =&gt; { document.getElementById('scrollbutton').onclick = smoothScrollToTop;}, 0); C++ #include&lt;cstdio&gt;#include&lt;iostream&gt;#include&lt;algorithm&gt;const int maxn=1050;const int maxm=20000010;int h,n,v[maxn],w[maxn],f[maxm],maxx;int main(){ cin&gt;&gt;h&gt;&gt;n; memset(f,0x3f,sizeof(f)); f[0]=0; for(int i=1;i&lt;=n;i++) cin&gt;&gt;v[i]&gt;&gt;w[i],maxx=max(v[i],maxx); for(int i=1;i&lt;=n;i++) { for(int j=v[i];j&lt;=h+maxx;j++) f[j]=min(f[j],f[j-v[i]]+w[i]); } int ans=inf; for(int i=h;i&lt;=h+maxx;i++) ans=min(ans,f[i]); cout&lt;&lt;ans&lt;&lt;endl; return 0;} PHP &lt;?phperror_reporting(0);if(!isset($_GET['num'])){ show_source(__FILE__);}else{ $str = $_GET['num']; $blacklist = [' ', '\\t', '\\r', '\\n','\\'', '&quot;', '`', '\\[', '\\]','\\$','\\\\','\\^']; foreach ($blacklist as $blackitem) { if (preg_match('/' . $blackitem . '/m', $str)) { die(&quot;what are you want to do?&quot;); } } eval('echo '.$str.';');}?&gt; JSON { &quot;name&quot;: &quot;hexo-theme-kaze&quot;, &quot;description&quot;: &quot;hexo-theme-kaze&quot;, &quot;author&quot;: &quot;theme-kaze&quot;, &quot;license&quot;: &quot;MIT&quot;, &quot;bugs&quot;: { &quot;url&quot;: &quot;https://github.com/theme-kaze/hexo-theme-Kaze/issues&quot; }, &quot;homepage&quot;: &quot;https://github.com/theme-kaze/hexo-theme-Kaze#readme&quot;} Python import requestsimport reimport sysimport osurl = 'https://twoshot.hgame.n3ko.co/'headers = { 'Content-Type': 'application/x-www-form-urlencoded', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',}p = requests.get(url+'random.php?times=228', headers=headers)randnum = re.findall(r'\\d*', p.text)[1:-2:2]firstnum = randnum[0]lastnum = randnum[227]content = os.popen(&quot;python3 ./reverse_mt_rand.py &quot; + firstnum+' '+lastnum+' 0 0').read()q = requests.post(url+'/verify.php', headers=headers, data={'ans': content})print(q.text) Go // backup my secret key into DB_, err = db.Exec(fmt.Sprintf(`INSERT INTO secret(secret) VALUES('%s');`, secretKey))if err != nil { return err}if u.TotalBalance &gt; 999999 || win { flag = os.Getenv(&quot;flag&quot;)} else { flag = &quot;nothing to show, to be a winner&quot;} function changePrismTheme(e) { let text = e.target.innerHTML; let linkList = document.head.getElementsByTagName('link'); for(let item of linkList) { if(item.dataset.prism) { document.head.removeChild(item); } } let link = document.createElement('link'); link.rel = 'stylesheet'; link.dataset.prism = text; if(text === 'default') { text = 'prism'; } else { text = 'prism-' + text; } link.href='/js/lib/prism/' + text +'.min.css'; document.head.appendChild(link); } setTimeout(() => { let buttonList = document.getElementsByClassName('postbutton'); Array.prototype.forEach.call(buttonList, item => { item.onclick = changePrismTheme; }); }, 0);","link":"/2020/08/03/highlight/"},{"title":"主题使用文档","text":"主题简介 Kaze是基于Hexo博客引擎的响应式主题，由 theme-kaze 开发维护 使用须知 文档并不包含所有配置项，其他选项请参考主题配置和 Hexo文档，如有配置上的其他疑问可以在issue中提出 安装主题 直接通过npm/yarn安装 在 Hexo 5.0 及以上版本中您可以直接通过输入 npm install hexo-theme-kaze 直接安装主题 其他方式 如果您有其他需要可以在 your site/themes 下输入 git clone https://github.com/theme-kaze/hexo-theme-Kaze.git 或者下载主题压缩包在 your site/themes 下解压，并且将 hexo-theme-kaze 重命名为 kaze 重命名为 kaze 是为了与通过 npm/yarn 下载保持一致性 持续升级 在Hexo5.0及以上版本中您可以在站点根目录中新建 _config.kaze.yml 进行自定义配置，具体优先级参见相关文档 在Hexo其他版本中您可以在站点目录 source 文件夹中新建 _data 文件夹并在期中新建 kaze_config.yml 复制主题配置文件的基本配置，然后就可以进行自定义配置，kaze-config.yml 设置优先于主题配置设置 之后直接采取 git pull 或覆盖主题目录等方式便可直接升级无需替换配置 请关注Release中的Breaking Changes会提示对配置造成不下兼容的改动 请自行更改 f(x;μ,σ2)=1σ2πe−12(x−μσ)2(1)f(x;\\mu,\\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2 } \\tag{1} f(x;μ,σ2)=σ2π​1​e−21​(σx−μ​)2(1) Test 总体 多语言 主题支持i18n国际化，目前支持英文与简体中文，有其他语言支持可提交PR。在站点配置文件中找到language即可修改语言 language: en 设置英文，language: zh-CN 设置简体中文 懒加载 在主题配置选项 lazyload 中设置 enable: true 开启懒加载（默认开启） loadingImg 中可以设置加载图，可以在post中设置 banner_img_set 进行覆盖实现缩略图效果 文件压缩 主题自带文件压缩，可以配合GZIP或其他优化手段提高网页访问速度，默认关闭，可以在 minify: enable: false css: true js: true html: true 中进行配置 需要注意开启压缩后会影响 hexo g 的性能 主题颜色 本主题中大部分颜色都可以在主题配置选项 color 中进行配置 字号与字体 在主题配置选项 font 中可以设置 font-size 和 font-family og meta 元素 Open graph 是由facebook推出的技术，可以帮助您在各类支持该技术的社交媒体上显示网页卡片 og: enable: true title: true url: true image: true description: true article: true enable 开启功能（默认开启） title… 请参见官网文档 静态资源 主题大部分第三方资源索引都可以在主题配置选项 cdn 中进行配置，可以自定义其他资源路径 动画 在主题配置文件中 animation config 调整主题动画效果，目前仅支持控制回到顶部动画的开关 图标 主题图标依赖于 iconfont，内置了一部分社交图标，您可以自定义其他icon文件或者解决方案来添加自定义图标 站点访问量统计 访问量统计目前仅支持不蒜子 footer: statistics: enable: false type: busuanzi # now version only supports busuanzi pv: enable: true style: 本站总访问量{}次 # the style will be shown as $1{pv}$2 uv: enable: true style: 本站总访客数{}次 enable 开启访问量统计（默认关闭） type 目前仅支持busuanzi pv 访问总量统计 uv 用户总量统计 数据分析 analytics: enable: false type: google # google google: id: enable 开启分析支持（默认关闭） type 目前仅支持 google google.id 有关谷歌分析的具体使用说明和 id 使用可以参考谷歌文档 首页 文章头图 在文章 Front-matter 中 banner_img 可以设置首页头图 小组件 widgets: showWidgetsMobiles: &quot;none&quot; showWidgetsMobiles : 在窄屏幕上是否显示小组件，none 关闭（默认），flex 开启 关于 关于页面需要自行创建，在站点 source 中新建 about 文件夹并在文件夹内创建 index.md，该文件至少需要包含 # at ${yoursite}/about/index.md---title: 关于layout: about--- 社交链接 在主题配置中 # about page configabout: description: description social_links: - { icon: icon, link: your_links } - ...more 在主题配置文件中按如上格式填写 social_links 即可创建社交链接，icon 相关可以参见图标 友链 友链格式按如下填写即可生成友链页面 links: example-name-1: url: https://example.com avatar: https://example.com/avatar.jpg example-name-2: url: https://example.com avatar: https://example.com/avatar.jpg 文章页 搜索功能 search: enable: true path: search.json field: posts searchContent: true enable 开启搜索功能（默认开启） path 文件名称（暂无用处） field 需要搜索的范围，支持 posts | pages | all searchContent 搜索文件是否包含正文内容（不建议开启，包含所有文章内容这样会使得搜索文件异常巨大）替代方案是搜索分类标签或使用algolia等第三方搜索服务（Todo） 目录 主题目录通过Hexo原生函数生成，具体可参见 文档 toc: showListNumber: false maxDepth: 6 minDepth: 1 showListNumber 是否生成编号 maxDepth TOC最大深度 minDepth TOC最小深度 代码高亮 参见代码 高亮文档 数学公式 主题支持 mathjax 和 katex 两种渲染引擎，具体参见 相关文档 copyright copyright: enable: true writer: # if writer is empty we will use config.author as writer declare: 本博客所有文章除特别声明外，均采用&lt;a target=&quot;_blank&quot; rel=&quot;noopener&quot; href=&quot;https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh&quot;&gt;CC BY-NC-SA 4.0 协议&lt;/a&gt;。转载请注明出处！ style: warning enable 开起版权说明（默认开启） writer 作者id，如果不填则会使用主题配置author或站点配置author declare 版权声明具体内容，支持 html 语句 style 声明内容样式，与 note 样式相同 版权内容有三部分：作者、文章链接、版权声明 作者使用 writer 参数，文章链接基于站点配置文件中url参数生成，版权声明使用 declare 参数 Front-matter banner_img 设置文章与首页头图 banner_img_set 在图片加载时预先加载的图片，可以设置为 loading 图或缩略图等 excerpt 为文章设置在首页显示的简介，还可以通过 &lt;!--more--&gt; 来控制显示 评论 支持 valine，gitalk 和 livere 具体设置可参考主题配置文档说明和相关评论插件文档 字数统计 主题集成 hexo-wordcount 插件，在主题配置文件中设置 wordcount: enable: true 开启（默认开启） 图片画廊 图片画廊功能基于 fslightbox，在主题配置文件中设置 fslightbox: enable: true 开启（默认开启） 标签插件 主题集成了一些标签方便书写 note 在 markdown 文件中如下书写即可 {% note style %}...markdown content{% endnote %} 有五种样式可以选择，primary，success，info，warning，danger primary success info warning danger 备案信息 您可以在主题配置文件内增加您的备案信息。 footer: #------------------------ # 备案配置 # 请将公安备案的缩略图置于 ${yoursite}/img/beian.png RecordInfo: &quot;&quot; # '某ICP备xxx号' govRecordInfo: &quot;&quot; # '某公网安备xxx号' govRecordUrl: &quot;&quot; # 公网安备案信息地址 #------------------------","link":"/2020/08/03/document/"},{"title":"荷塘月色","text":"这几天心里颇不宁静。今晚在院子里坐着乘凉，忽然想起日日走过的荷塘，在这满月的光里，总该另有一番样子吧。月亮渐渐地升高了，墙外马路上孩子们的欢笑，已经听不见了；妻在屋里拍着闰儿，迷迷糊糊地哼着眠歌。我悄悄地披了大衫，带上门出去。 沿着荷塘，是一条曲折的小煤屑路。这是一条幽僻的路；白天也少人走，夜晚更加寂寞。荷塘四面，长着许多树，蓊蓊郁郁的。路的一旁，是些杨柳，和一些不知道名字的树。没有月光的晚上，这路上阴森森的，有些怕人。今晚却很好，虽然月光也还是淡淡的。 路上只我一个人，背着手踱着。这一片天地好像是我的；我也像超出了平常的自己，到了另一个世界里。我爱热闹，也爱冷静；爱群居，也爱独处。像今晚上，一个人在这苍茫的月下，什么都可以想，什么都可以不想，便觉是个自由的人。白天里一定要做的事，一定要说的话，现 在都可不理。这是独处的妙处，我且受用这无边的荷塘月色好了。 曲曲折折的荷塘上面，弥望的是田田的叶子。叶子出水很高，像亭亭的舞女的裙。层层的叶子中间，零星地点缀着些白花，有袅娜地开着的，有羞涩地打着朵儿的；正如一粒粒的明珠，又如碧天里的星星，又如刚出浴的美人。微风过处，送来缕缕清香，仿佛远处高楼上渺茫的歌声似的。这时候叶子与花也有一丝的颤动，像闪电般，霎时传过荷塘的那边去了。叶子本是肩并肩密密地挨着，这便宛然有了一道凝碧的波痕。叶子底下是脉脉的流水，遮住了，不能见一些颜色；而叶子却更见风致了。 月光如流水一般，静静地泻在这一片叶子和花上。薄薄的青雾浮起在荷塘里。叶子和花仿佛在牛乳中洗过一样；又像笼着轻纱的梦。虽然是满月，天上却有一层淡淡的云，所以不能朗照；但我以为这恰是到了好处——酣眠固不可少，小睡也别有风味的。月光是隔了树照过来的，高处丛生的灌木，落下参差的斑驳的黑影，峭楞楞如鬼一般；弯弯的杨柳的稀疏的倩影，却又像是画在荷叶上。塘中的月色并不均匀；但光与影有着和谐的旋律，如梵婀玲上奏着的名曲。 荷塘的四面，远远近近，高高低低都是树，而杨柳最多。这些树将一片荷塘重重围住；只在小路一旁，漏着几段空隙，像是特为月光留下的。树色一例是阴阴的，乍看像一团烟雾；但杨柳的丰姿，便在烟雾里也辨得出。树梢上隐隐约约的是一带远山，只有些大意罢了。树缝里也漏着一两点路灯光，没精打采的，是渴睡人的眼。这时候最热闹的，要数树上的蝉声与水里的蛙声；但热闹是它们的，我什么也没有。 忽然想起采莲的事情来了。采莲是江南的旧俗，似乎很早就有，而六朝时为盛；从诗歌里可以约略知道。采莲的是少年的女子，她们是荡着小船，唱着艳歌去的。采莲人不用说很多，还有看采莲的人。那是一个热闹的季节，也是一个风流的季节。梁元帝《采莲赋》里说得好： 于是妖童媛女，荡舟心许；鷁首徐回，兼传羽杯；棹将移而藻挂，船欲动而萍开。尔其纤腰束素，迁延顾步；夏始春余，叶嫩花初，恐沾裳而浅笑，畏倾船而敛裾。 可见当时嬉游的光景了。这真是有趣的事，可惜我们现 在早已无福消受了。 于是又记起，《西洲曲》里的句子： 采莲南塘秋，莲花过人头；低头弄莲子，莲子清如水。 今晚若有采莲人，这儿的莲花也算得“过人头”了；只不见一些流水的影子，是不行的。这令我到底惦着江南了。——这样想着，猛一抬头，不觉已是自己的门前；轻轻地推门进去，什么声息也没有，妻已睡熟好久了。 一九二七年七月，北京清华园。","link":"/2020/07/27/%E8%8D%B7%E5%A1%98%E6%9C%88%E8%89%B2/"},{"title":"海思NNIE之PFPLD训练与量化","text":"本文首发于我的知乎：https://zhuanlan.zhihu.com/p/120376337 之前写了关于海思NNIE的一些量化部署工作，笔者不才，文章没有写得很具体，有些内容并没有完全写在里面。好在目前看到了一些使用nniefacelib脱坑的朋友，觉得这个工程还是有些用的。为了完善这个工程，最近也增加一些一站式的解决方案。开始正题吧！ https://github.com/hanson-young/nniefacelib 1. 训练 PFLD 是一个精度高、速度快、模型小三位一体的人脸关键点检测算法。github上也有对其进行的复现工作，而这次要介绍的就是https://github.com/hanson-young/nniefacelib/blob/master/PFPLD/README.md 。 PFPLD （A Practical Facial Pose and Landmark Detector），对PFLD的微改版本，笔者对其进行了一些微小的改变，名字中间多了个”P“。其实是对pose branch进行了加强，同时让其关键点对遮挡、模糊、光照等复杂情况更加鲁棒。 黄色虚线囊括的是主分支网络，用于预测关键点的位置；绿色虚线囊括的是head pose辅助网络。在训练时预测人脸姿态，从而修改loss函数，使更加关注那些稀有的，还有姿态角度过大的样本，从而提高预测的精度。同等规模的网络，只要精度上去，必然是可以想到很多办法来降低计算量的。 直观感受，这个loss的设计模式本质上是一种对抗数据不均衡的表达，和focal loss思想是一致的。但这类思想并不是对于每种工作都能work，笔者曾经回答过类似的问题。 深度学习的多个loss如何平衡 &amp; 有哪些「魔改」损失函数，曾经拯救了你的深度学习模型？ 接下来将介绍一些笔者对其微改的地方： 在github上的代码分为了两个分支，下面单独做一下讲解 二、V1.1.1分支 用PRNet（https://github.com/YadiraF/PRNet）标注人脸图像的姿态数据，比原始通过solvePNP得到的效果要好很多，这也直接增强了模型对pose的支持。PRNet是一个非常优秀的3D人脸方面的项目。论文也写的很精彩，强烈推荐去看。目前在活体检测领域用其渲染的depth map作为伪标签进行训练，已经成为了一种标配性的存在。所以当人脸姿态估计算法性能接近于它，证明训练的姿态已经非常不错了。如果想要得到更好的表现，用更加特殊的方法采集人脸姿态数据进行炼丹也是行得通的（吐槽：大部分开源姿态数据标注规范并不统一）。 在整个实验中pfld loss收敛速度比较慢，慢也是有原因的，过于重点关注少量复杂的样本，会使得对整体的grad调节不明显，因此对多种loss（mse, pfld, smoothl1 and wing）进行了对比，结果得出，wing loss的效果更加明显，收敛速度也比较快。 改进了pfld网络结构，让关键点和姿态角度都能回归的比较好，将landmarks branch合并到pose branch中。由于两个任务相关性较强， 这样做可以让其更加充分的影响。对于这种多任务之间正向促进的例子，通过对网络结构以及辅助监督信号的改进，可以使其结果并不会太过于依赖loss函数的设计。这并不是笔者在的主观判断，感兴趣，可以参考我之前的一个回答，如有不同之处，欢迎一起讨论相关话题。 深度学习的多个loss如何平衡 &amp; 有哪些「魔改」损失函数，曾经拯救了你的深度学习模型？ 三，master分支 分支V1.1.1也存在一些问题，比如最大的一个问题就是闭眼的时候效果并不好，显然眼睛部分的关键点已经出现过拟合了。而master分支改进的效果为右列，得到了一些优化。 我们也发现一个规律，分别用WLFW 98个点，LAPA 106个点的数据集进行训练，闭眼效果都不行，而300WLP上的却没问题。这或许是一个通病，我也试了其他的算法，也有这些问题，比如 https://github.com/zeusees/HyperLandmark 原版本的PFLD 为什么会出现这个现象呢？这其实和训练数据集里面闭眼图片的数量过少有关系，加强眼部的训练并不能抵抗这种情况，因为不是一个维度的事情，最佳的方式依然是添加闭眼数据。同时也建议大家在制作数据集的时候考虑数据的均衡性 详细的讨论如下： https://github.com/hanson-young/nniefacelib/issues/13 另外一个问题是PRNet的pose预测在抬头时候不精确，因此V1.1.1中直接用PRNet去标注也不是一种理想方式 为了解决上面两个后面发现的问题，为了解决数据的均衡性，我们挑选出LAPA 106个关键点的数据集中闭眼的数据，加入WLFW中用于解决闭眼问题，引入300WLP用于解决pose问题），新的代码对 dataloader以及wing loss函数进行了优化，目前数据集已经整理好放出来了！请使用PFPLD-Dataset数据集进行训练！欢迎尝鲜！ 虽然融合大法好，但是，我们发现一个问题，网上已经开源了很多的关键点数据集，而这些数据集的标注点数并不一致，有没有办法将这些数据都利用起来呢？比如上述WLFW 98点、LAPA 106点、300WLP 68点。答案是肯定的，我们可以有针对性的选择其中的一些点对眼睛、鼻子、轮廓进行单独加强！在实际操作上，LAPA相对于WLFW增加了部分关键点，而且是完全兼容的，这样就可以先对WLFW和LAPA进行一个融合，姑且叫做WLFW2。 而融合300WLP就会面临一些困难，因为点的对应上没法完全兼容，因此需要参考标注标准挑选一些能兼容的点进行训练。下面是对loss的一些修改。 class WingLoss(nn.Module): def __init__(self): super(WingLoss, self).__init__() self.num_lds = 98 self.size = self.num_lds * 2 self.w = 10.0 self.s = 5.0 # 挑选出WLFW2中的眼睛关键点 self.eye_index = [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75] # 挑选出300WLP中与WLFW2想对应的部分关键点 self.pts_68_to_98 = [33,34,35,36,37,42,43,44,45,46,51,52,53,54,55,56,57,58,59,60,64,68,72,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95] self.pts_onehot = [i for i in range(98)] for i in self.pts_onehot: if i in self.pts_68_to_98: self.pts_onehot[i] = True else: self.pts_onehot[i] = False self.epsilon = 2.0 def forward(self, attribute_gt, landmark_gt, euler_angle_gt, type_flag , angle, landmarks, train_batchsize): landms_const = torch.tensor(-2).cuda() pose_68landms_const = torch.tensor(0).cuda() # WLFW2中98个点的loss pos1 = type_flag == landms_const landm_p = landmarks.reshape(-1, self.num_lds, 2)[pos1] landm_t = landmark_gt.reshape(-1, self.num_lds, 2)[pos1] lds_98_loss = 0 if landm_p.shape[0] &gt; 0: x = landm_t - landm_p c = self.w * (1.0 - math.log(1.0 + self.w / self.epsilon)) absolute_x = torch.abs(x) weight_attribute = landm_p*0.0 + 1.0 weight_attribute[:,self.eye_index] *= self.s absolute_x = torch.mul(absolute_x, weight_attribute) lds_losses = torch.where(self.w &gt; absolute_x, self.w * torch.log(1.0 + absolute_x / self.epsilon), absolute_x - c) lds_98_loss = torch.mean(torch.sum(lds_losses, axis=[1, 2]), axis=0) # 300WLP 中部分关键点的loss和姿态的loss pos2 = type_flag == pose_68landms_const pose_p = angle.view(-1, 3)[pos2] pose_t = euler_angle_gt.view(-1, 3)[pos2] pose_loss = 0 if pose_p.shape[0] &gt; 0: pose_loss = F.smooth_l1_loss(pose_p, pose_t, reduction='mean') landm_p = landmarks.reshape(-1, self.num_lds, 2)[pos2] landm_t = landmark_gt.reshape(-1, self.num_lds, 2)[pos2] lds_68_loss = 0 if landm_p.shape[0] &gt; 0: landm_p = landm_p[:, self.pts_onehot] landm_t = landm_t[:, self.pts_onehot] x = landm_t - landm_p absolute_x = torch.abs(x) c = self.w * (1.0 - math.log(1.0 + self.w / self.epsilon)) lds_losses = torch.where(self.w &gt; absolute_x, self.w * torch.log(1.0 + absolute_x / self.epsilon), absolute_x - c) lds_68_loss = torch.mean(torch.sum(lds_losses, axis=[1, 2]), axis=0) return lds_98_loss + lds_68_loss, pose_loss*1000 疫情当下，口罩遮挡，玄学优化，美图共赏 四、量化 过去一周，笔者对训练代码进行了整理，完成了多种版本的转换工作，包括 pytorch caffe ncnn nnie 听说有小伙伴将这套模型跑到了ios上，说不定之后会放出来。 扯了一大堆，那开始介绍下本文最核心的NNIE。有首先我们要选择一个比较优秀的训练框架，比如，我们选择了pytorch。然后要将模型转换为caffe，那我们选择了onnx作为过度环节 python convert_to_onnx.pypython3 -m onnxsim ./models/onnx/checkpoint_epoch_final.onnx ./models/onnx/pfpld.onnxcd cvtcaffepython convertCaffe.py 可以看得出来，经过了很少的步骤，一个被图优化过的caffe模型就出来了，包括merge bn，inplace等优化，&quot;工具人&quot;onnx在其中起到了很重要的作用。 之前也提到过batchnorm会对精度造成一些不知所以的影响，所以 无论我们遇到什么困难，都不要怕，微笑着面对它，消除恐惧的最好办法就是避开恐惧，避开，才会胜利，加油，奥利给！ 另外，笔者自己训练模型的时候是不会考虑减均值这种操作的，只会做data_scale处理，为什么这么做，因为放弃思考真得很香。记性不好，不想遇到问题的时候去查，也不太相信减均值能带来明显收益！","link":"/2021/06/20/%E6%B5%B7%E6%80%9DNNIE%E4%B9%8BPFPLD%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%87%8F%E5%8C%96/"},{"title":"海思NNIE之RetinaFace量化部署","text":"本文首发于我的知乎：https://zhuanlan.zhihu.com/p/111399987 0. 前言 接上一期 海思NNIE之Mobilefacenet量化部署 文章。 关于上述内容，还是得到了一些认可，索性把人脸全家福奉上了，Github地址如下： https://github.com/hanson-young/nniefacelib nniefacelib是一个在海思35xx系列芯片上运行的人脸算法库，目前集成了mobilefacenet和retinaface。 后期也会融合一些其他经典的模型，目的也是总结经验，让更多人早日脱离苦海。欢迎关注！ 这篇的话，就讲下RetinaFace的量化和部署吧！相信很多同学在转换的时候吃了苦头，那我们就来宣泄一下吧！ RetinaFace是目前非常优秀的开源人脸检测算法 https://github.com/deepinsight/insightface/tree/master/RetinaFace 实测效果确实很棒，鲁棒性强，关键点准，能够识别比较复杂场景下的人脸，甚至于比我采用私有数据集训练的mtcnn某些方面还要强，有全局感受野的模型在复杂人脸上的检测效果会有很大优势。很多同学在海思上也有往mtcnn方向下功夫的，其实可以走得通，但现实是，海思弱鸡的CPU算力还是留给其他业务逻辑吧，直接用one-stage的方法也是很香的。另外one stage模型优化起来也是很粗暴的。天下武功，唯快不破，tracking也可以告别光流，而去使用sort，甚至一些场景直接用deepsort去解决switch IDs。综上，将RetinaFace这类模型放上海思的板子上是有巨大的优势。 我参考的代码和模型来自于 https://github.com/Charrin/RetinaFace-Cpp 1. 特殊层Crop 在nnie上，有的层是不支持的，这个官方SDK里有专门的章节描述，其中retinaface就有一个Crop层是不支持的，需要手动写插件，但这当然是不能接受的，毕竟一想到将计算量转移部分到CPU上就觉得不踏实，后期肯定会给自己带来麻烦。那么对于这个问题，我们首先要做的就是明白这个层的作用。 Caffe中crop_layer描述是这样的： 数据以blobs形式存在，blob是一个四维结构的Tensor（batch size, channels, height, width），假设crop_layer的输入（bottom）有两个，A和B，输出（top）为C A——要进行裁剪的bottom，假设A的size为（20，50，512，512） B——裁剪的参考输入，size为（20，10，256，256） C——输出top，它是在A的基础上按照B的size裁剪而来，所以输出C的size和B是一样的 layer { name: &quot;crop0&quot; type: &quot;Crop&quot; bottom: &quot;A&quot; bottom: &quot;B&quot; top: &quot;C&quot;} 知道Crop的原理和模型训练的流程就能明白，作者利用Crop的原理其实就是解决多尺度输入的问题，可以在比赛或实验中去利用多尺度提升性能，但实际在跑视频流或者做应用的时候并不需要每一次都跑尺寸不同的图像，一般都是根据需要，设定单一尺度。 所以我采取了比较暴力的做法，就是直接在mnet.prototxt中两处地方去除了Crop层（Fig.2.2，Fig.2.3），然后寻找几组固定的输入比如512x512、640x640、768x768、1024x1024等，只要保证在Crop操作之前两个层的维度是一致的就没有什么问题！ 然后去掉prototxt里的两处crop后，结构图如下： 修改完prototxt后就可以直接在RuyiStudio中使用Marker进行模型拓扑图像的观测，调节data的输入尺度，可以测试出几组能满足条件的size，这样之后的转换问题应该就不会很大了！如Figure2.4和Figure2.5所示。 3. ReShape prototxt中存在多处reshape的地方，如下图，需要将dim: 1改成dim: 0，原因很简单，在量化的时候会报一个错误，就是只能设置CHW三个维度，没有N，这个维度的设定应该是为了让NNIE多张输入的时候方便操作四维数据，可以参考样例中fasterRCNN中的写法，进行必要调整。 4. 小雷区 还有一个地方比较坑，也是CNN_convert_bin_and_print_featuremap.py代码的问题，因为这个代码里面没有将BGR转化为RGB，而cfg里写的并没有用到，之前测试的图像是agedb中的图像，刚好是一张三通道一致的灰度图，因此需要在读取图像的时候注意一下，加上这句代码 img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) 5. 大雷区 开开心心的走到了这一步，那其实最大的问题才刚刚抵达战场。就是前方这个错误，让人魂牵梦萦的Error。最后的解决方案也让人无法入睡。 其实解决方法就是改变prototxt中layer的顺序，对你没听过。修改过后的prototxt已经被放在nniefacelib了 其中有几个点是目前还想不明白的，非常希望得到大佬的指点： pycaffe上运行确实是没问题的，结果也正确。NNIE mapper工具可以生成inst.mk，生成过程中没有如上错误，但是在板端测试的时候输出是错误的；NNIE mapper工具也无法生成func.mk，但可以生成mapper_quant，生成过程中有如上错误。 有的地方batchnorm层之前精度还是很高的，过batchnorm后精度就完全对不上了，有的地方没问题，本以为是batchnorm里的参数有问题，但和mxnet版本的model param进行了对比后并没有发现任何异常。最后输出层的结果却也是精损极小，意料之中。这个问题在转其他模型的时候也遇到过。（PS：要不直接merge bn吧，这个问题就可以跳过了） 经过逐层的调试虽然解决了模型生成的问题，但是也仅仅改变了prototxt layer的顺序，按道理说并不应该出现这样的情况。 虽然存在一些不可告知的问题，但是也给了我们一个调试经验，在遇到输出不对的时候不要慌，先从对的那一层开始，修改prototxt，删除之后的layer开始调试，直到查明原因为止。如果还是没办法，重新训练也是个好方法。 5. 量化仿真 做完了上面的步骤就可以进行量化了，目前做的测试显示，低精度的量化在landmarks和bbox的回归上精度还是偏低的，但够用，部署起来输出的误差也就像素级误差，而高精度模式下自然也不用担心，只是效率会稍微降低一点，我也给出一个在3516DV300上的大概结果吧！图像尺寸640x640，算上后处理，高精度大概40ms，低精度20多ms。 如果对文章有什么不理解或者疑惑的地方，欢迎到文章开头的知乎文章的讨论区给我留言哦。","link":"/2021/06/20/%E6%B5%B7%E6%80%9DNNIE%E4%B9%8BRetinaFace%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"title":"海思NNIE之Mobilefacenet量化部署","text":"本文首发于我的知乎：https://zhuanlan.zhihu.com/p/107548509 0. 前言 那就直接更新下整理好的代码链接吧！ https://github.com/hanson-young/nniefacelib 当您点进这篇文章，我想肯定不需要过多的去向您介绍华为海思35xx系列芯片的型号参数或者强大之处。另外这个教程也是建立已经配置好环境，并掌握Ruyi Studio的基本使用前提下的。如果还没有跑过其中的一些sample，网上也有一些教程，推荐看刘山老师的博客（地址为：https://blog.csdn.net/avideointerfaces/article/details/88585654）。 这篇文章的目录如下： 简介 目录结构 mobilefacenet.cfg文件的配置 生成NNIE mk模型 Vector Comparision NNIE mobilefacenet板上特征提取 附录 1. 简介 海思35xx系列芯片对比起nvidia TX2、Intel Movidius神经计算棒等一众边缘计算产品，有其惊艳的地方，因其集成了强大的算力模块，集成度和功能模块齐全，最重要的是成本低，成为了安防行业的首选芯片。但是也有一些麻烦的地方，主要是在于其开发难度的提高，大家都是摸着石头过河（3288老玩家转行也是能体会到痛苦的）。在转自己的模型时，坑比想象的要多，并且海思官方SDK也存在一些错误之处，让人很难捉摸，所以有时候需要自己多去独立思考。这次我记录了在转换人脸识别模型mobilefacenet（https://github.com/deepinsight/insightface）下了比较坑的三个点，毕竟是个新玩意儿，多半是版本发布时候不统一造成的： CNN_convert_bin_and_print_featuremap.py 代码出现错误，cfg中的【image_list】这个字段并没有在代码中出现，代码中只有【image_file】，因此需要修改这一地方。 CNN_convert_bin_and_print_featuremap.py和Get Caffe Output这里的预处理方式都是先乘以【data_scale】，再减均值【mean_file】，而在量化生成 .mk 文件时却是先减均值再乘以scale的。 量化需要使用多张图片，而CNN_convert_bin_and_print_featuremap.py各层产生的feature仅仅是一张图片，这在做【Vector Comparision】时候就难以清楚的明白到底最后mk文件是第几张图像。 2. 目录结构 3. mobilefacenet.cfg文件的配置 可以从github上下载mxnet2caffe的mobilefacenet模型（https://github.com/honghuCode/mobileFacenet-ncnn/tree/feature/mobilefacenet-mxnet2caffe），首先需要修改mobilefacenet.prototxt（https://github.com/honghuCode/mobileFacenet-ncnn/blob/feature/mobilefacenet-mxnet2caffe/mobilefacenet.prototxt）的输入层以符合NNIE caffe网络的结构标准： 而量化mk使用的【mean_file】pixel_mean.txt是特别需要注意的 我从agedb_30人脸数据库里面挑选了10张图像来做量化处理，为什么需要多张量化，请参考文章https://zhuanlan.zhihu.com/p/58182172，我们选择【10.jpg】来做 【Vector Comparision】，其实就是imageList.txt里的排列在最后的那张图片 具体配置如下： [prototxt_file] ./mark_prototxt/mobilefacenet_mark_nnie_20190723102335.prototxt[caffemodel_file] ./data/face/mobilefacenet.caffemodel[batch_num] 256[net_type] 0[sparse_rate] 0[compile_mode] 0[is_simulation] 0[log_level] 3[instruction_name] ./data/face/mobilefacenet_inst[RGB_order] RGB[data_scale] 0.0078125[internal_stride] 16[image_list] ./data/face/images/imageList20190723102419.txt[image_type] 1[mean_file] ./data/face/pixel_mean.txt[norm_type] 5 4. 生成NNIE mk模型 Start [RuyiStudio Wk NNIE Mapper] [E:\\Code\\nnie\\windows\\RuyiStudio-2.0.31\\workspace\\HeilsFace\\mobilefacenet.cfg] HeilsFace (2019-07-23 10:48:17)Mapper Version 1.1.2.0_B050 (NNIE_1.1) 1812171743151709begin net parsing.....end net parsingbegin prev optimizing........end prev optimizing....begin net quantalizing(GPU)........................**********************************************************WARNING: file: Inference::computeNonlinearQuantizationDelta line: 92data containing only zeros; set max value to 1e-6.**********************************************************WARNING: file: Inference::computeNonlinearQuantizationDelta line: 92data containing only zeros; set max value to 1e-6........................................end quantalizingbegin optimizing.....end optimizingbegin NNIE[0] mem allocation.......end NNIE[0] memory allocatingbegin NNIE[0] instruction generating.................end NNIE[0] instruction generatingbegin parameter compressing.....end parameter compressingbegin compress index generating....end compress index generatingbegin binary code generating............................................................................................................................................................................................................................................................................................................................................................end binary code generatingbegin quant files writing....end quant files writing===============E:\\Code\\nnie\\windows\\RuyiStudio-2.0.31\\workspace\\HeilsFace\\mobilefacenet.cfg Successfully!=============== 结束之后会生成： mobilefacenet_inst.wk文件 mapper_quant文件夹，里面有量化输出的结果，如图 Fig.4.1，也就是./data/face/images/10.jpg 记住，mk量化过程在【mapper_quant】文件夹中生成的features是最后一张图片的inference结果，这也是文章最开始说的第三个存在问题的地方 5. Vector Comparision 这一步，主要就是对比量化前后模型输出的精度损失，最重要的就是要debug一遍CNN_convert_bin_and_print_featuremap.py 因为这个脚本里确实藏了很多雷，我们先要比较原框架原模型inference的结果与这一脚本得出来的结果是否一致，如果存在不一致的情况，需要去核查一遍原因 文章开篇说到的第一个问题点 CNN_convert_bin_and_print_featuremap.py 中加载了mobilefacenet.cfg文件，但脚本中并不存在【image_list】这个字段，取而代之的是【image_file】这个字段 生成NNIE mk中，mobliefacenet.cfg 的【image_list】： CNN_convert_bin_and_print_featuremap.py 中加载.cfg代码片段： 因此需要根据实际情况修改 mobliefacenet.cfg ，这里最好是复制一份新的，旧的用于生成NNIE wk，在复制后的mobliefacenet.cfg中修改一下： 另外，我们需要特别注意预处理这一个环节，如文章开篇所阐述的第二点 我们注意到这里，data是uint8类型的array，是先乘以了【data_scale】的，也就是说和NNIE 生成wk中的操作顺序是不一致的。 (data - 128.0) * 0.0078125 &lt;==&gt; data * 0.0078125 - 1 因此这里需要做的修改就是需要将【mean_file】pixel_mean.txt修改为 修改完以上，然后直接运行代码，将最终模型提取的features fc1_output0_128_caffe.linear.float和caffe_forward.py（https://github.com/honghuCode/mobileFacenet-ncnn/blob/feature/mobilefacenet-mxnet2caffe/caffe_forward.py）中的进行比对，如果以上都没问题，可以看到结果是几乎一致的 caffe_forward.py生成的结果： [-0.82475293 -0.33066949 -0.9848339 2.44199681 0.41715512 0.67809981 0.29879519 1.14293635 -0.42905819 0.32940909 -1.20455348 1.01217067 0.83146936 -0.84349883 -1.49177814 -0.91509151 -1.39441037 0.00413842 0.97043389 -1.77688181 0.28639579 -1.06645989 -0.8570649 -2.09743094 -0.1394622 -1.15035641 -0.81590587 -3.93798804 -0.35600579 1.90367532 1.27935755 -2.07778478 -0.42563218 0.06624207 1.02597868 -0.52002895 -0.905873 -0.41364694 -1.40032899 -1.37654066 0.03066693 -0.18659458 -1.53931415 -0.55896652 2.42570448 -0.3044413 0.18183242 0.50442797 -2.36735368 -0.12376076 0.15200013 0.13939141 0.56305337 -0.10047323 1.50704932 0.05429612 -1.97527623 -0.75790995 1.89399767 0.56089604 -2.34883094 0.22600658 1.00399816 -0.55099922 1.77083731 0.10722937 2.21140814 0.06182361 0.03354079 0.97481596 -2.00423741 0.73168194 -1.79977489 -0.85182911 -0.06020565 -0.14835797 -1.93012297 -3.09269047 -0.60087907 -1.02915597 1.40985525 1.85411906 -1.21282506 -2.53264689 -0.63467324 -1.15255475 -0.59994221 0.21181655 1.30336523 -1.73625863 0.00861333 0.99906266 1.90666902 0.51179212 0.62143475 1.01997399 -1.65181398 1.55190873 0.43448481 -0.85371047 -0.68216199 1.28038061 0.4629558 -0.59671575 1.00122356 1.74233603 1.50384009 0.49827856 0.67030573 -1.20388556 1.00168729 -0.71768999 1.06416941 -2.55346298 -1.85579956 -2.18774438 -1.79652691 1.50856853 2.10628557 1.12313557 2.76396179 0.60242128 0.0550903 -1.31998527 -0.6896565 -0.07160443 1.21242583 -1.06733179] CNN_convert_bin_and_print_featuremap.py生成的结果（由于特征值太多，就不一一打印出来了）： 然后再生成，并进行【Vector Comparision】，量化终于成功了 6. NNIE mobilefacenet板上特征提取 做完了模型的量化，就可以进行仿真或者是在板子上进行实际测试了，这一步的坑并不是很多，主要还是得靠一些编程技巧了，建议熟悉C语言，这部分要熟悉sample代码，如果说非常熟悉c/c混编，也可以使用c。 6.1 修改例程 这里参考了https://blog.csdn.net/u011728480/article/details/92069793，其写法几乎一致，如下Fig.6.1 Fig.6.2是我所修改的代码片段，找到smp/a7_linux/mpp/sample/svp/nnie/sample/sample_nnie.c中该函数 void SAMPLE_SVP_NNIE_Cnn(void) 只用修改了该函数的前后两处代码 我们调用了 SAMPLE_SVP_NNIE_PrintReportResult 函数输出两个结果报表文件，结果分析当中会用到 seg0_layer38_output0_inst.linear.hexseg0_layer3605_output0_inst.linear.hex 整段函数代码参见文章末尾【附录】 6.2 bgr文件的生成 注意到上文中我使用了pcSrcFile，这也是例程中主流的格式bgr，那么我们一般的图片都是.jpeg格式的，为了更好的利用NNIE，所以就需要利用opencv来转化以下。 首先.bgr文件是可以由opencv Mat转换的，但完成转换代码的编写之前我们必须清楚像素的空间排列顺序。注意，以下转换代码简单采用像素复制，并没有考虑优化，运行会比较慢！参考博客 .bgr ==&gt; BBBBBB…GGGGGG…RRRRRR cv::Mat ==&gt; BGRBGRBGR…BGRBGRBGR .bgr --&gt; cv::Mat /*bgr格式 转 cv::Mat代码 */int bgr2mat(cv::Mat&amp; img, int width, int height, int channel, const char* pth){ if (pth) { FILE* fp; unsigned char *img_data = NULL; unsigned char *img_data_conv = NULL; img_data = (unsigned char*)malloc(sizeof(unsigned char) * width * height * channel); //unsigned char img_data[300 * 300 * 3]; img_data_conv = (unsigned char*)malloc(sizeof(unsigned char) * width * height * channel); fp = fopen(pth, &quot;rb&quot;); if (!fp) { return 0; } fread(img_data, 1, width * height * channel, fp); fclose(fp); for (size_t k = 0; k &lt; channel; k++) for (size_t i = 0; i &lt; height; i++) for (size_t j = 0; j &lt; width; j++) img_data_conv[channel * (i * width + j) + k] = img_data[k * height * width + i * width + j]; img = cv::Mat(height, width, CV_8UC3, img_data_conv); //free(img_data_conv); //img_data_conv = NULL; free(img_data); img_data = NULL; return 1; } return 0;} cv::Mat --&gt;.bgr /*cv::Mat 转 bgr格式代码 */int mat2bgr(cv::Mat&amp; img, const char* bgr_path){ if (bgr_path) { FILE* fp = fopen(bgr_path, &quot;wb&quot;); int step = img.step; int h = img.rows; int w = img.cols; int c = img.channels(); std::cout &lt;&lt; step&lt;&lt; std::endl; for (int k = 0; k &lt; c; k++) for (int i = 0; i &lt; h; i++) for (int j = 0; j &lt; w; j++) { //两种写法 //fwrite(&amp;img.data[i*step + j * c + k], sizeof(uint8_t), 1, fp); fwrite(&amp;img.data[c*(i * w + j) + k], sizeof(uint8_t), 1, fp); } fclose(fp); //cv::Mat tmp; //bgr2mat(tmp, w, h, 3, bgr_path); //cv::imshow(&quot;tmp&quot;, tmp); //cv::waitKey(0); return 1; } return 0;} 6.3 模型额外问题 pc上运行 E:\\Code\\nnie\\software\\sample_simulator\\Release\\sample_simulator.exe 板上运行 /nfsroot/Hi3516CV500_SDK_V2.0.1.0/smp/a7_linux/mpp/sample/svp/nnie # ./sample_nnie_main 4 可能会出现如下（Fig.6.5，Fig.6.6）错误，原因是生成NNIE wk文件的mapper工具有版本要求，下面错误当中使用的nnie mapper 版本是V1.1.2.0，而指令仿真或者是板上的SDK是V1.2的，解决办法就是使用nnie mapper V1.2版本重新生成一下wk模型，如（Fig.6.7），生成inst/chip.wk的时间比较久，在我机器上大概要2个小时，因为inst.wk实际上是需要进行参数压缩和二进制代码生成，这可能也是inst.mk比func.wk文件大的原因（如Fig.6.8），而生成func.wk的时间会比较短，建议在PC上调试的时候选择func/simulation模型 6.4 运行结果及分析 修改完sample_nnie.c中的代码后，在宿主机上进行make，然后到海思板子上运行可执行文件即可 拷贝出生成的两个打印报表文件到Ruyi studio，进行比对测试 seg0_layer38_output0_inst.linear.hexseg0_layer3605_output0_inst.linear.hex 如Fig.6.10，Fig.6.11，虽然说板上和仿真情况下还是会有一定的差别，但总体的误差是比较小的，基本可以接受，如果无法接受，可以尝试int16模型 7. 附录 void SAMPLE_SVP_NNIE_Cnn(void){ HI_CHAR *pcSrcFile = &quot;./data/nnie_image/rgb_planar/10.bgr&quot;; HI_CHAR *pcModelName = &quot;./data/nnie_model/face/mobilefacenet_inst.wk&quot;; HI_U32 u32PicNum = 1; HI_S32 s32Ret = HI_SUCCESS; SAMPLE_SVP_NNIE_CFG_S stNnieCfg = {0}; SAMPLE_SVP_NNIE_INPUT_DATA_INDEX_S stInputDataIdx = {0}; SAMPLE_SVP_NNIE_PROCESS_SEG_INDEX_S stProcSegIdx = {0}; /*Set configuration parameter*/ stNnieCfg.pszPic= pcSrcFile; stNnieCfg.u32MaxInputNum = u32PicNum; //max input image num in each batch stNnieCfg.u32MaxRoiNum = 0; stNnieCfg.aenNnieCoreId[0] = SVP_NNIE_ID_0;//set NNIE core s_stCnnSoftwareParam.u32TopN = 5; /*Sys init*/ SAMPLE_COMM_SVP_CheckSysInit(); /*CNN Load model*/ SAMPLE_SVP_TRACE_INFO(&quot;Cnn Load model!\\n&quot;); s32Ret = SAMPLE_COMM_SVP_NNIE_LoadModel(pcModelName,&amp;s_stCnnModel); SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,CNN_FAIL_0,SAMPLE_SVP_ERR_LEVEL_ERROR, &quot;Error,SAMPLE_COMM_SVP_NNIE_LoadModel failed!\\n&quot;); /*CNN parameter initialization*/ /*Cnn software parameters are set in SAMPLE_SVP_NNIE_Cnn_SoftwareParaInit, if user has changed net struct, please make sure the parameter settings in SAMPLE_SVP_NNIE_Cnn_SoftwareParaInit function are correct*/ SAMPLE_SVP_TRACE_INFO(&quot;Cnn parameter initialization!\\n&quot;); s_stCnnNnieParam.pstModel = &amp;s_stCnnModel.stModel; s32Ret = SAMPLE_SVP_NNIE_Cnn_ParamInit(&amp;stNnieCfg,&amp;s_stCnnNnieParam,&amp;s_stCnnSoftwareParam); SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,CNN_FAIL_0,SAMPLE_SVP_ERR_LEVEL_ERROR, &quot;Error,SAMPLE_SVP_NNIE_Cnn_ParamInit failed!\\n&quot;); /*record tskBuf*/ s32Ret = HI_MPI_SVP_NNIE_AddTskBuf(&amp;(s_stCnnNnieParam.astForwardCtrl[0].stTskBuf)); SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,CNN_FAIL_0,SAMPLE_SVP_ERR_LEVEL_ERROR, &quot;Error,HI_MPI_SVP_NNIE_AddTskBuf failed!\\n&quot;); /*Fill src data*/ SAMPLE_SVP_TRACE_INFO(&quot;Cnn start!\\n&quot;); stInputDataIdx.u32SegIdx = 0; stInputDataIdx.u32NodeIdx = 0; s32Ret = SAMPLE_SVP_NNIE_FillSrcData(&amp;stNnieCfg,&amp;s_stCnnNnieParam,&amp;stInputDataIdx); SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,CNN_FAIL_1,SAMPLE_SVP_ERR_LEVEL_ERROR, &quot;Error,SAMPLE_SVP_NNIE_FillSrcData failed!\\n&quot;); /*NNIE process(process the 0-th segment)*/ stProcSegIdx.u32SegIdx = 0; s32Ret = SAMPLE_SVP_NNIE_Forward(&amp;s_stCnnNnieParam,&amp;stInputDataIdx,&amp;stProcSegIdx,HI_TRUE); SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,CNN_FAIL_1,SAMPLE_SVP_ERR_LEVEL_ERROR, &quot;Error,SAMPLE_SVP_NNIE_Forward failed!\\n&quot;); /*Software process*/ /*if user has changed net struct, please make sure SAMPLE_SVP_NNIE_Cnn_GetTopN function's input datas are correct*/ s32Ret = SAMPLE_SVP_NNIE_Cnn_GetTopN(&amp;s_stCnnNnieParam,&amp;s_stCnnSoftwareParam); SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,CNN_FAIL_1,SAMPLE_SVP_ERR_LEVEL_ERROR, &quot;Error,SAMPLE_SVP_NNIE_CnnGetTopN failed!\\n&quot;); /*Print result*/ SAMPLE_SVP_TRACE_INFO(&quot;Cnn result:\\n&quot;); s32Ret = SAMPLE_SVP_NNIE_Cnn_PrintResult(&amp;(s_stCnnSoftwareParam.stGetTopN), s_stCnnSoftwareParam.u32TopN); SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,CNN_FAIL_1,SAMPLE_SVP_ERR_LEVEL_ERROR, &quot;Error,SAMPLE_SVP_NNIE_Cnn_PrintResult failed!\\n&quot;); /*Print results*/ { printf(&quot;features:\\n{\\n&quot;); printf(&quot;stride: %d\\n&quot;,s_stCnnNnieParam.astSegData[0].astDst[0].u32Stride); printf(&quot;blob type :%d\\n&quot;,s_stCnnNnieParam.astSegData[0].astDst[0].enType); printf(&quot;{\\n\\tc :%d&quot;, s_stCnnNnieParam.astSegData[0].astDst[0].unShape.stWhc.u32Chn); printf(&quot;\\n\\th :%d&quot;, s_stCnnNnieParam.astSegData[0].astDst[0].unShape.stWhc.u32Height); printf(&quot;\\n\\tw :%d \\n}\\n&quot;, s_stCnnNnieParam.astSegData[0].astDst[0].unShape.stWhc.u32Width); HI_S32* ps32Score = (HI_S32* )((HI_U8* )s_stCnnNnieParam.astSegData[0].astDst[0].u64VirAddr); printf(&quot;blobs fc1:\\n[&quot;); for(HI_U32 i = 0; i &lt; 128; i++) { printf(&quot;%f ,&quot;,*(ps32Score + i) / 4096.f); } printf(&quot;]\\n}\\n&quot;); } s32Ret = SAMPLE_SVP_NNIE_PrintReportResult(&amp;s_stCnnNnieParam); SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret, CNN_FAIL_1, SAMPLE_SVP_ERR_LEVEL_ERROR,&quot;Error,SAMPLE_SVP_NNIE_PrintReportResult failed!&quot;);CNN_FAIL_1: /*Remove TskBuf*/ s32Ret = HI_MPI_SVP_NNIE_RemoveTskBuf(&amp;(s_stCnnNnieParam.astForwardCtrl[0].stTskBuf)); SAMPLE_SVP_CHECK_EXPR_GOTO(HI_SUCCESS != s32Ret,CNN_FAIL_0,SAMPLE_SVP_ERR_LEVEL_ERROR, &quot;Error,HI_MPI_SVP_NNIE_RemoveTskBuf failed!\\n&quot;);CNN_FAIL_0: SAMPLE_SVP_NNIE_Cnn_Deinit(&amp;s_stCnnNnieParam,&amp;s_stCnnSoftwareParam,&amp;s_stCnnModel); SAMPLE_COMM_SVP_CheckSysExit();}","link":"/2021/06/20/%E6%B5%B7%E6%80%9DNNIE%E4%B9%8BMobilefacenet%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2/"}],"tags":[{"name":"mathjax","slug":"mathjax","link":"/tags/mathjax/"},{"name":"katex","slug":"katex","link":"/tags/katex/"},{"name":"数学公式","slug":"数学公式","link":"/tags/%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"},{"name":"代码高亮","slug":"代码高亮","link":"/tags/%E4%BB%A3%E7%A0%81%E9%AB%98%E4%BA%AE/"},{"name":"散文","slug":"散文","link":"/tags/%E6%95%A3%E6%96%87/"},{"name":"nniefacelib","slug":"nniefacelib","link":"/tags/nniefacelib/"}],"categories":[{"name":"文档","slug":"文档","link":"/categories/%E6%96%87%E6%A1%A3/"},{"name":"演示","slug":"演示","link":"/categories/%E6%BC%94%E7%A4%BA/"},{"name":"技术","slug":"技术","link":"/categories/%E6%8A%80%E6%9C%AF/"}]}